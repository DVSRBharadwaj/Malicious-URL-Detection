{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zea1JgAx4VR"
      },
      "source": [
        "# Deep Reinforcement Learning for Malicious URL Detection\n",
        "\n",
        "## Overview\n",
        "This notebook trains a Deep Q-Network (DQN) model for multi-class URL classification (Benign, Malware, Phishing, Spam, Defacement) using Google Colab's GPU. It combines a Transformer model for feature extraction with a DRL agent for adaptive decision-making.\n",
        "\n",
        "## Prerequisites\n",
        "- Set runtime to GPU: `Runtime > Change runtime type > Hardware accelerator > GPU`.\n",
        "- Upload `kaggle.json` for dataset access (get from Kaggle > Account > API > Create New API Token).\n",
        "- Obtain Google Safe Browsing API key from [Google Cloud Console](https://console.cloud.google.com/) and store in Colab Secrets.\n",
        "- Ensure stable internet for API calls and package installation.\n",
        "\n",
        "## Steps\n",
        "1. Install dependencies and restart runtime.\n",
        "2. Configure API keys and mount Google Drive.\n",
        "3. Fetch and preprocess data from URLhaus, Google Safe Browsing, and Kaggle.\n",
        "4. Define a custom Gym environment for URL classification.\n",
        "5. Train the DQN model on GPU.\n",
        "6. Save models to Google Drive.\n",
        "7. Evaluate model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOz2IBmTx4VT",
        "outputId": "9a572a10-39e1-4d80-8b9f-a731030fb16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pandas version: 2.2.2\n",
            "Requests version: 2.32.3\n",
            "TensorFlow version: 2.18.0\n",
            "Gym version: 0.25.2\n",
            "Error importing libraries: name 'transformers' is not defined\n",
            "Please go to Runtime > Restart runtime, then rerun this cell to confirm imports.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install Dependencies\n",
        "#!pip install --force-reinstall stable-baselines3==2.3.2 gym==0.25.2 requests==2.32.3 pandas==2.2.2 transformers==4.41.0 kaggle==1.6.14 shimmy\n",
        "#!pip install transformers==4.41.0\n",
        "# Import libraries\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import tensorflow as tf\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Verify installations\n",
        "try:\n",
        "    print(\"Pandas version:\", pd.__version__)\n",
        "    print(\"Requests version:\", requests.__version__)\n",
        "    print(\"TensorFlow version:\", tf.__version__)\n",
        "    print(\"Gym version:\", gym.__version__)\n",
        "    print(\"Transformers version:\", transformers.__version__)\n",
        "    print(\"Dependencies installed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error importing libraries: {e}\")\n",
        "\n",
        "# Important: Restart the runtime to apply new package versions\n",
        "print(\"Please go to Runtime > Restart runtime, then rerun this cell to confirm imports.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "DjuFIbLNx4VU",
        "outputId": "c58b45a8-ca28-4915-98a2-8083f449535c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Upload kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba1ea991-9bf8-478f-9b7e-28444c26e264\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ba1ea991-9bf8-478f-9b7e-28444c26e264\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.14)\n",
            "ref                                                             title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "rakeshkapilavai/extrovert-vs-introvert-behavior-data            Extrovert vs. Introvert Behavior Data               31KB  2025-06-13 14:26:48          18603        405  1.0              \n",
            "bismasajjad/global-ai-job-market-and-salary-trends-2025         Global AI Job Market & Salary Trends 2025          517KB  2025-06-01 07:20:49           6907        111  0.9411765        \n",
            "adilshamim8/social-media-addiction-vs-relationships             Students' Social Media Addiction                     8KB  2025-05-10 14:38:02          17898        272  1.0              \n",
            "prajwaldongre/loan-application-and-transaction-fraud-detection  Loan Application & Transaction: Fraud Detection      8MB  2025-06-10 08:44:19           1182         24  1.0              \n",
            "shalmamuji/personality-prediction-data-introvert-extrovert      Personality prediction data | introvert extrovert  160KB  2025-06-12 10:38:45            727         25  1.0              \n",
            "therohithanand/used-car-price-prediction                        Used Car Price Prediction                          141KB  2025-06-09 08:04:12           2060         29  1.0              \n",
            "brendanartley/openfwi-preprocessed-72x72                        OpenFWI Preprocessed 72x72                          21GB  2025-06-02 17:25:58           3717         61  1.0              \n",
            "samanfatima7/2020-2025-apple-stock-dataset                      2020-2025 Apple Stock Dataset                       51KB  2025-06-03 11:57:03           1001         23  0.9411765        \n",
            "sahilislam007/shopping-trends-and-customer-behaviour-dataset    Shopping Trends And Customer Behaviour Dataset      78KB  2025-05-25 13:51:18           1913         29  0.9411765        \n",
            "sahilislam007/sales-dataset                                     Sales Dataset                                        9KB  2025-05-27 07:28:19           1811         30  1.0              \n",
            "abhishekdave9/digital-habits-vs-mental-health-dataset           Screen Time Impact on Mental Health                546KB  2025-06-13 11:42:28            522         24  0.9411765        \n",
            "adilshamim8/rock-paper-scissors                                 Rock Paper Scissors SXSW: Hand Gesture Detection   201MB  2025-05-28 04:09:22           1773         78  1.0              \n",
            "chaitu20/ipl-dataset2008-2025                                   IPL Dataset(2008-2025)                               6MB  2025-06-06 17:04:29           1252         28  1.0              \n",
            "hbugrae/best-selling-steam-games-of-all-time                    Best-Selling Steam Games of All Time               154KB  2025-06-12 11:24:15            870         22  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                           Melbourne Housing Snapshot                         451KB  2018-06-05 12:52:24         182448       1637  0.7058824        \n",
            "skullagos5246/upi-transactions-2024-dataset                     UPI Transactions 2024 Dataset                        5MB  2025-06-14 21:39:13            598         21  1.0              \n",
            "anirudhsub/twizzlerdata                                         Popularity of Twizzlers Dataset                     996B  2025-06-13 22:01:17            260         23  1.0              \n",
            "datasnaek/youtube-new                                           Trending YouTube Video Statistics                  201MB  2019-06-03 00:56:47         273960       5687  0.7941176        \n",
            "zynicide/wine-reviews                                           Wine Reviews                                        51MB  2017-11-27 17:08:04         327444       3741  0.7941176        \n",
            "datasnaek/chess                                                 Chess Game Dataset (Lichess)                         3MB  2017-09-04 03:09:09          55521       1356  0.8235294        \n",
            "Kaggle API working.\n",
            "Google Safe Browsing API key loaded.\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Configure API Keys and Google Drive\n",
        "# Mount Google Drive to save models\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Upload Kaggle API key\n",
        "print(\"Upload kaggle.json\")\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Test Kaggle API\n",
        "try:\n",
        "    !kaggle datasets list\n",
        "    print(\"Kaggle API working.\")\n",
        "except Exception as e:\n",
        "    print(f\"Kaggle API error: {e}\")\n",
        "\n",
        "# Set Google Safe Browsing API key (use Colab Secrets)\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('google_search_api')\n",
        "    print(\"Google Safe Browsing API key loaded.\")\n",
        "except Exception as e:\n",
        "    GOOGLE_API_KEY = 'YOUR_GOOGLE_SAFE_BROWSING_API_KEY'  # Replace with your key if not using Secrets\n",
        "    print(f\"Error loading API key from Secrets: {e}. Using placeholder key (replace it).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgHycyr12lAQ",
        "outputId": "b89f4405-4ade-42ea-cdc2-d67591ae135f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (80.9.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Automatically fetching benign URLs from Cisco Umbrella Top 1M...\n",
            "Fetched 250 benign URLs from Cisco Umbrella Top 1M.\n",
            "Fetching malicious URLs from URLhaus...\n",
            "Automatic download succeeded: 500 URLs loaded (Umbrella: 250, URLhaus: 250).\n",
            "Skipping Safe Browsing API (invalid key or no URLs). Using default labels.\n",
            "Loaded 500 URLs with labels (Primary: 250, URLhaus: 250).\n"
          ]
        }
      ],
      "source": [
        "# Updated Cell-3\n",
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Install and enable ipywidgets for interactive UI\n",
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "# Initialize variables\n",
        "urls, labels = [], []\n",
        "urls_primary, labels_primary = [], []\n",
        "urls_urlhaus, labels_urlhaus = [], []\n",
        "\n",
        "# Attempt automatic download of Cisco Umbrella Top 1M and URLhaus\n",
        "auto_success = False\n",
        "try:\n",
        "    print(\"Automatically fetching benign URLs from Cisco Umbrella Top 1M...\")\n",
        "    umbrella_url = \"http://s3-us-west-1.amazonaws.com/umbrella-static/top-1m.csv.zip\"\n",
        "    response = requests.get(umbrella_url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    with open(\"top-1m.csv.zip\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    with zipfile.ZipFile(\"top-1m.csv.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    df_umbrella = pd.read_csv(\"top-1m.csv\", names=[\"rank\", \"domain\"])\n",
        "    urls_primary = [\"http://\" + domain for domain in df_umbrella[\"domain\"].head(250)]  # Limit to 250\n",
        "    labels_primary = [0] * len(urls_primary)  # Benign (0)\n",
        "    print(f\"Fetched {len(urls_primary)} benign URLs from Cisco Umbrella Top 1M.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching Cisco Umbrella Top 1M: {e}\")\n",
        "    print(\"Using hardcoded benign URLs as fallback.\")\n",
        "    urls_primary = [\n",
        "        \"http://google.com\", \"http://wikipedia.org\", \"http://youtube.com\", \"http://amazon.com\", \"http://facebook.com\",\n",
        "        \"http://twitter.com\", \"http://linkedin.com\", \"http://microsoft.com\", \"http://apple.com\", \"http://netflix.com\"\n",
        "    ] * 25  # 250 URLs\n",
        "    labels_primary = [0] * len(urls_primary)\n",
        "    print(f\"Loaded {len(urls_primary)} hardcoded benign URLs.\")\n",
        "\n",
        "# Fetch malicious URLs from URLhaus\n",
        "def fetch_urlhaus_data():\n",
        "    try:\n",
        "        print(\"Fetching malicious URLs from URLhaus...\")\n",
        "        response = requests.get('https://urlhaus.abuse.ch/downloads/text/', timeout=10)\n",
        "        response.raise_for_status()\n",
        "        urls = [url for url in response.text.splitlines() if url and not url.startswith('#')]\n",
        "        return urls[:250], [1] * min(250, len(urls))  # Malware (1), limit to 250\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching URLhaus data: {e}\")\n",
        "        return [], []\n",
        "\n",
        "urls_urlhaus, labels_urlhaus = fetch_urlhaus_data()\n",
        "\n",
        "# Combine data if both sources succeeded\n",
        "if urls_primary and urls_urlhaus:\n",
        "    urls = urls_primary + urls_urlhaus\n",
        "    labels = labels_primary + labels_urlhaus\n",
        "    auto_success = True\n",
        "    print(f\"Automatic download succeeded: {len(urls)} URLs loaded (Umbrella: {len(urls_primary)}, URLhaus: {len(urls_urlhaus)}).\")\n",
        "\n",
        "# Interactive UI if automatic download fails\n",
        "if not auto_success:\n",
        "    print(\"Automatic download failed. Select a dataset to download:\")\n",
        "    dataset_options = [\n",
        "        \"Umbrella Top 1M + URLhaus (Benign + Malware)\",\n",
        "        \"CIC-MalURL-2020 (Manual Upload from https://www.unb.ca/cic/datasets/)\",\n",
        "        \"PhishTank (Phishing URLs)\"\n",
        "    ]\n",
        "    dataset_dropdown = widgets.Dropdown(\n",
        "        options=dataset_options,\n",
        "        description=\"Dataset:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    download_button = widgets.Button(\n",
        "        description=\"Download\",\n",
        "        button_style=\"primary\",\n",
        "        tooltip=\"Click to download the selected dataset\"\n",
        "    )\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_download_button_clicked(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            selected_dataset = dataset_dropdown.value\n",
        "            global urls, labels, urls_primary, labels_primary, urls_urlhaus, labels_urlhaus\n",
        "            urls, labels = [], []\n",
        "            urls_primary, labels_primary = [], []\n",
        "            urls_urlhaus, labels_urlhaus = [], []\n",
        "\n",
        "            if selected_dataset == \"Umbrella Top 1M + URLhaus (Benign + Malware)\":\n",
        "                try:\n",
        "                    print(\"Fetching benign URLs from Cisco Umbrella Top 1M...\")\n",
        "                    response = requests.get(umbrella_url, timeout=10)\n",
        "                    response.raise_for_status()\n",
        "                    with open(\"top-1m.csv.zip\", \"wb\") as f:\n",
        "                        f.write(response.content)\n",
        "                    with zipfile.ZipFile(\"top-1m.csv.zip\", \"r\") as zip_ref:\n",
        "                        zip_ref.extractall()\n",
        "                    df_umbrella = pd.read_csv(\"top-1m.csv\", names=[\"rank\", \"domain\"])\n",
        "                    urls_primary = [\"http://\" + domain for domain in df_umbrella[\"domain\"].head(250)]\n",
        "                    labels_primary = [0] * len(urls_primary)\n",
        "                    print(f\"Fetched {len(urls_primary)} benign URLs from Cisco Umbrella Top 1M.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fetching Cisco Umbrella Top 1M: {e}\")\n",
        "                    urls_primary = [\n",
        "                        \"http://google.com\", \"http://wikipedia.org\", \"http://youtube.com\", \"http://amazon.com\", \"http://facebook.com\",\n",
        "                        \"http://twitter.com\", \"http://linkedin.com\", \"http://microsoft.com\", \"http://apple.com\", \"http://netflix.com\"\n",
        "                    ] * 25\n",
        "                    labels_primary = [0] * len(urls_primary)\n",
        "                    print(f\"Loaded {len(urls_primary)} hardcoded benign URLs.\")\n",
        "                urls_urlhaus, labels_urlhaus = fetch_urlhaus_data()\n",
        "\n",
        "            elif selected_dataset == \"CIC-MalURL-2020 (Manual Upload from https://www.unb.ca/cic/datasets/)\":\n",
        "                print(\"Please upload the CIC-MalURL-2020 CSV file.\")\n",
        "                uploaded = files.upload()\n",
        "                if uploaded:\n",
        "                    try:\n",
        "                        csv_file = list(uploaded.keys())[0]\n",
        "                        df = pd.read_csv(csv_file)\n",
        "                        if 'type' in df.columns:\n",
        "                            df['label'] = df['type'].map({'benign': 0, 'malware': 1, 'phishing': 2, 'spam': 3, 'defacement': 4})\n",
        "                        elif 'label' in df.columns and df['label'].dtype == object:\n",
        "                            df['label'] = df['label'].map({'Benign': 0, 'Malware': 1, 'Phishing': 2, 'Spam': 3, 'Defacement': 4, 'benign': 0, 'malware': 1, 'phishing': 2, 'spam': 3, 'defacement': 4})\n",
        "                        elif 'label' in df.columns and df['label'].dtype in [int, float]:\n",
        "                            pass\n",
        "                        else:\n",
        "                            raise ValueError(\"No recognizable 'type' or 'label' column in CSV\")\n",
        "                        urls_primary = df['url'].tolist()[:500]\n",
        "                        labels_primary = df['label'].tolist()[:500]\n",
        "                        print(f\"Uploaded CIC dataset loaded: {len(urls_primary)} URLs.\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading CIC dataset: {e}\")\n",
        "\n",
        "            elif selected_dataset == \"PhishTank (Phishing URLs)\":\n",
        "                try:\n",
        "                    print(\"Fetching phishing URLs from PhishTank...\")\n",
        "                    response = requests.get('http://data.phishtank.com/data/online-valid.csv.gz', timeout=10)\n",
        "                    df = pd.read_csv(response.content, compression='gzip')\n",
        "                    urls_primary = df['url'].tolist()[:250]\n",
        "                    labels_primary = [2] * len(urls_primary)  # Phishing (2)\n",
        "                    print(f\"Fetched {len(urls_primary)} phishing URLs from PhishTank.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fetching PhishTank data: {e}\")\n",
        "\n",
        "            urls = urls_primary + urls_urlhaus\n",
        "            labels = labels_primary + labels_urlhaus\n",
        "            if urls:\n",
        "                print(f\"Loaded {len(urls)} URLs with labels (Primary: {len(urls_primary)}, URLhaus: {len(urls_urlhaus)}).\")\n",
        "            else:\n",
        "                print(\"No URLs loaded. Using synthetic fallback dataset.\")\n",
        "                urls = [\n",
        "                    'http://example.com', 'http://safe-site.org', 'http://legit-site.net', 'http://trusted-page.com', 'http://benign-url.org',\n",
        "                    'http://malware-site.com', 'http://virus-download.net', 'http://trojan-page.org', 'http://malicious-code.com', 'http://harmful-site.net',\n",
        "                    'http://phish-site.com', 'http://fake-login-page.org', 'http://scam-bank.com', 'http://phishing-url.net', 'http://credential-stealer.org',\n",
        "                    'http://spam-offer.com', 'http://unwanted-ad.net', 'http://fake-deal.org', 'http://spam-promotion.com', 'http://adware-site.net',\n",
        "                    'http://defaced-site.org', 'http://hacked-page.com', 'http://vandalized-url.net', 'http://defacement-page.org', 'http://compromised-site.com'\n",
        "                ] * 4  # 100 URLs\n",
        "                labels = [0] * 20 + [1] * 20 + [2] * 20 + [3] * 20 + [4] * 20\n",
        "                print(f\"Fallback dataset loaded: {len(urls)} URLs.\")\n",
        "\n",
        "    download_button.on_click(on_download_button_clicked)\n",
        "    display(dataset_dropdown, download_button, output)\n",
        "\n",
        "# Google Safe Browsing\n",
        "def fetch_safe_browsing_data(urls):\n",
        "    url = f'https://safebrowsing.googleapis.com/v4/threatMatches:find?key={GOOGLE_API_KEY}'\n",
        "    payload = {\n",
        "        'client': {'clientId': 'mycompany', 'clientVersion': '1.0'},\n",
        "        'threatInfo': {\n",
        "            'threatTypes': ['MALWARE', 'SOCIAL_ENGINEERING', 'UNWANTED_SOFTWARE', 'POTENTIALLY_HARMFUL_APPLICATION'],\n",
        "            'platformTypes': ['ANY_PLATFORM'],\n",
        "            'threatEntryTypes': ['URL'],\n",
        "            'threatEntries': [{'url': url} for url in urls[:100]]\n",
        "        }\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        threats = response.json().get('matches', [])\n",
        "        labels = [0] * len(urls)\n",
        "        for threat in threats:\n",
        "            url = threat['threat']['url']\n",
        "            threat_type = threat['threatType']\n",
        "            if url in urls:\n",
        "                idx = urls.index(url)\n",
        "                if threat_type == 'MALWARE':\n",
        "                    labels[idx] = 1\n",
        "                elif threat_type == 'SOCIAL_ENGINEERING':\n",
        "                    labels[idx] = 2\n",
        "                elif threat_type == 'UNWANTED_SOFTWARE':\n",
        "                    labels[idx] = 3\n",
        "                else:\n",
        "                    labels[idx] = 4\n",
        "        return labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Safe Browsing data: {e}\")\n",
        "        return [0] * len(urls)\n",
        "\n",
        "if GOOGLE_API_KEY != userdata.get('google_search_api') and urls:\n",
        "    labels = fetch_safe_browsing_data(urls)\n",
        "else:\n",
        "    print(\"Skipping Safe Browsing API (invalid key or no URLs). Using default labels.\")\n",
        "\n",
        "# Filter invalid URLs\n",
        "valid_data = [(url, label) for url, label in zip(urls, labels) if url and isinstance(url, str) and pd.notna(label)]\n",
        "urls, labels = zip(*valid_data) if valid_data else ([], [])\n",
        "\n",
        "print(f\"Loaded {len(urls)} URLs with labels (Primary: {len(urls_primary)}, URLhaus: {len(urls_urlhaus)}).\")\n",
        "if len(urls) == 0:\n",
        "    print(\"Warning: No valid URLs loaded. Check dataset sources and API keys.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqO4ZogKF5O1",
        "outputId": "f89dd723-faf1-41cf-8254-79c47bec3ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "print(len(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "bHz4sZG77uEe",
        "outputId": "4ded12f5-3c53-4165-d079-18b103173fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload the following albert-base-v2 files: config.json, pytorch_model.bin, tokenizer_config.json, spiece.model\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9940810e-3bfa-46f0-9831-cb216970945e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9940810e-3bfa-46f0-9831-cb216970945e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving config.json to config.json\n",
            "Saving pytorch_model.bin to pytorch_model.bin\n",
            "Saving spiece.model to spiece.model\n",
            "Saving tokenizer_config.json to tokenizer_config.json\n",
            "Uploaded model files to /content/albert-base-v2.\n",
            "Loaded albert-base-v2 successfully.\n",
            "Environment initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# Updated Cell-4\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Upload model files\n",
        "def upload_model_files():\n",
        "    print(\"Upload the following albert-base-v2 files: config.json, pytorch_model.bin, tokenizer_config.json, spiece.model\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        model_dir = \"/content/albert-base-v2\"\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        for filename, content in uploaded.items():\n",
        "            with open(os.path.join(model_dir, filename), 'wb') as f:\n",
        "                f.write(content)\n",
        "        print(f\"Uploaded model files to {model_dir}.\")\n",
        "        return model_dir\n",
        "    raise RuntimeError(\"No files uploaded. Please upload the required model files.\")\n",
        "\n",
        "# URL environment\n",
        "class URLEnvironment(gym.Env):\n",
        "    def __init__(self, urls, labels, model_dir):\n",
        "        super(URLEnvironment, self).__init__()\n",
        "        self.urls = urls\n",
        "        self.labels = labels\n",
        "        self.current_idx = 0\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True, local_files_only=True)\n",
        "            self.model = AutoModel.from_pretrained(model_dir, local_files_only=True)\n",
        "            print(\"Loaded albert-base-v2 successfully.\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load albert-base-v2 from {model_dir}: {e}\")\n",
        "        self.action_space = gym.spaces.Discrete(5)  # 0: benign, 1: malware, 2: phishing, 3: spam, 4: defacement\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(768,), dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        url = self.urls[self.current_idx]\n",
        "        inputs = self.tokenizer(url, return_tensors=\"pt\", max_length=128, truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        reward = 1.0 if action == self.labels[self.current_idx] else -1.0\n",
        "        self.current_idx = (self.current_idx + 1) % len(self.urls)\n",
        "        done = self.current_idx == 0\n",
        "        return embedding, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_idx = 0\n",
        "        url = self.urls[self.current_idx]\n",
        "        inputs = self.tokenizer(url, return_tensors=\"pt\", max_length=128, truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        return embedding\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "# Initialize environment\n",
        "model_dir = upload_model_files()\n",
        "try:\n",
        "    env = URLEnvironment(urls, labels, model_dir)\n",
        "    print(\"Environment initialized successfully.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error initializing environment: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j4Xuzl7x4VV",
        "outputId": "aa28931e-4e52-4570-d2eb-35fdfa575c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 500      |\n",
            "|    ep_rew_mean      | 230      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 8        |\n",
            "|    time_elapsed     | 225      |\n",
            "|    total_timesteps  | 2000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.00073  |\n",
            "|    n_updates        | 474      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 500      |\n",
            "|    ep_rew_mean      | 346      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 8        |\n",
            "|    time_elapsed     | 449      |\n",
            "|    total_timesteps  | 4000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.000355 |\n",
            "|    n_updates        | 974      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 500      |\n",
            "|    ep_rew_mean      | 382      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 9        |\n",
            "|    time_elapsed     | 661      |\n",
            "|    total_timesteps  | 6000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.000104 |\n",
            "|    n_updates        | 1474     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 500      |\n",
            "|    ep_rew_mean      | 402      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 9        |\n",
            "|    time_elapsed     | 865      |\n",
            "|    total_timesteps  | 8000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.00012  |\n",
            "|    n_updates        | 1974     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 500      |\n",
            "|    ep_rew_mean      | 414      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 9        |\n",
            "|    time_elapsed     | 1075     |\n",
            "|    total_timesteps  | 10000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.000163 |\n",
            "|    n_updates        | 2474     |\n",
            "----------------------------------\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Train DQN Model\n",
        "try:\n",
        "    dqn_model = DQN(\n",
        "        'MlpPolicy',\n",
        "        env,\n",
        "        verbose=1,\n",
        "        learning_rate=1e-3,\n",
        "        buffer_size=10000,\n",
        "        batch_size=32,\n",
        "        device='cuda' if tf.config.list_physical_devices('GPU') else 'cpu'\n",
        "    )\n",
        "    dqn_model.learn(total_timesteps=10000)\n",
        "    print(\"Training completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCzpvhNXx4VV",
        "outputId": "3971b160-6492-4a11-f666-3844d7a2f82e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models saved to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Save Models\n",
        "try:\n",
        "    dqn_model.save('/content/drive/MyDrive/drl_url_detector.zip')\n",
        "    #model.save_pretrained('/content/drive/MyDrive/transformer_url_detector')\n",
        "    #tokenizer.save_pretrained('/content/drive/MyDrive/transformer_url_detector')\n",
        "    print(\"Models saved to Google Drive.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving models: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kU85ixwx4VV",
        "outputId": "b1f49032-165b-49f5-b9b5-e53fb838f793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 95.00%\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Evaluate Model\n",
        "try:\n",
        "    correct = 0\n",
        "    total = 100\n",
        "    obs = env.reset()\n",
        "    for _ in range(total):\n",
        "        action, _ = dqn_model.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if reward > 0:\n",
        "            correct += 1\n",
        "    accuracy = correct / total * 100\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "except Exception as e:\n",
        "    print(f\"Error during evaluation: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjpUb5apPnD-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
